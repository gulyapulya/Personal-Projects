{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from string import punctuation\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test and Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape :  (1999, 2)\n",
      "Test shape :  (500, 2)\n"
     ]
    }
   ],
   "source": [
    "#Loading Data\n",
    "\n",
    "train = pd.read_csv('products_sentiment_train.tsv', delimiter='\\t')\n",
    "test = pd.read_csv('products_sentiment_test.tsv', delimiter='\\t')\n",
    "\n",
    "print 'Train shape : ', train.shape\n",
    "print 'Test shape : ', test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Naming the columns\n",
    "\n",
    "train.columns = ['text', 'target']\n",
    "test.drop('Id', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>i downloaded a trial version of computer assoc...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>the wrt54g plus the hga7t is a perfect solutio...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>i dont especially like how music files are uns...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>i was using the cheapie pail ... and it worked...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>you can manage your profile , change the contr...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  target\n",
       "0  i downloaded a trial version of computer assoc...       1\n",
       "1  the wrt54g plus the hga7t is a perfect solutio...       1\n",
       "2  i dont especially like how music files are uns...       0\n",
       "3  i was using the cheapie pail ... and it worked...       1\n",
       "4  you can manage your profile , change the contr...       1"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Check\n",
    "\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>so , why the small digital elph , rather than ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3/4 way through the first disk we played on it...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>better for the zen micro is outlook compatibil...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6 . play gameboy color games on it with goboy .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>likewise , i 've heard norton 2004 professiona...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text\n",
       "0  so , why the small digital elph , rather than ...\n",
       "1  3/4 way through the first disk we played on it...\n",
       "2  better for the zen micro is outlook compatibil...\n",
       "3    6 . play gameboy color games on it with goboy .\n",
       "4  likewise , i 've heard norton 2004 professiona..."
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Check\n",
    "\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Overview by the Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of texts with the target == 1 :  1273\n",
      "The percentage : 63.7\n",
      "\n",
      "Random 5 samples:\n",
      "[\"while this phone obviously doesn 't have the same quality construction as motorola does , the nokia 6600 is one of the better phones i 've used .\"]\n",
      "[\"i 've only had it a week , but so far , everything about this camera is making me happy . \"]\n",
      "['it holds plenty of songs .']\n",
      "['freebies you get : a cradle with detachable stand and belt clip , some thing very handy and useful .']\n",
      "['- the replacable battery is great since once it eventually wears out ( as all lithium batteries do ) , you will be able to buy another easily . ']\n"
     ]
    }
   ],
   "source": [
    "print 'Number of texts with the target == 1 : ', train[train['target']==1].shape[0]\n",
    "print ('The percentage : %0.1f' % (100.*train[train['target']==1].shape[0]/train.shape[0]))\n",
    "print '\\nRandom 5 samples:'\n",
    "for t in train.loc[train.target==1, ['text']].sample(5).values:\n",
    "    print t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of texts with the target == 0 :  726\n",
      "The percentage : 36.3\n",
      "\n",
      "Random 5 samples:\n",
      "['and supply those stupid white headphones .']\n",
      "['the cut-outs for the controls is not thought out as there is too much material in the way to adequately access the controls , especially the scroll wheel . ']\n",
      "['it would not hang up on calls .']\n",
      "['because steve jobs is a twisted individual and he made sure that he ruined this device by giving it an unreplacable 18 month battery .']\n",
      "['controls are a bit awkward . ']\n"
     ]
    }
   ],
   "source": [
    "print 'Number of texts with the target == 0 : ', train[train['target']==0].shape[0]\n",
    "print ('The percentage : %0.1f' % (100.*train[train['target']==0].shape[0]/train.shape[0]))\n",
    "print '\\nRandom 5 samples:'\n",
    "for t in train.loc[train.target==0, ['text']].sample(5).values:\n",
    "    print t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN\n",
    "\n",
    "def preprocess(text):\n",
    "    specials = [\"’\", \"‘\", \"´\", \"`\"]\n",
    "    contraction_mapping = {\"n't\": \"not\", \"'t\": \"not\", \"'d\": \"would\", \"'ll\": \"will\", \"'s\": \"is\", \n",
    "                       \"'ve\": \"have\", \"'m\": \"am\", \"'re\": \"are\", \"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"'cause\": \"because\", \"could've\": \n",
    "                       \"could have\", \"couldn't\": \"could not\", \"didn't\": \"did not\",  \"doesn't\": \"does not\", \"don't\": \n",
    "                       \"do not\", \"hadn't\": \"had not\", \"hasn't\": \"has not\", \"haven't\": \"have not\", \"he'd\": \"he would\",\n",
    "                       \"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \n",
    "                       \"how will\", \"how's\": \"how is\",  \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \n",
    "                       \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \"i'd\": \"i would\", \"i'd've\": \n",
    "                       \"i would have\", \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \n",
    "                       \"isn't\": \"is not\", \"it'd\": \"it would\", \"it'd've\": \"it would have\", \"it'll\": \"it will\", \n",
    "                       \"it'll've\": \"it will have\",\"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\", \n",
    "                       \"mayn't\": \"may not\", \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \n",
    "                       \"might not have\", \"must've\": \"must have\", \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \n",
    "                       \"needn't\": \"need not\", \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\", \"oughtn't\": \n",
    "                       \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \"sha'n't\": \"shall not\", \n",
    "                       \"shan't've\": \"shall not have\", \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \n",
    "                       \"she will\", \"she'll've\": \"she will have\", \"she's\": \"she is\", \"should've\": \"should have\", \n",
    "                       \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\"so's\": \"so as\", \n",
    "                       \"this's\": \"this is\",\"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\", \n",
    "                       \"there'd\": \"there would\", \"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \n",
    "                       \"here is\",\"they'd\": \"they would\", \"they'd've\": \"they would have\", \"they'll\": \"they will\", \n",
    "                       \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \n",
    "                       \"to have\", \"wasn't\": \"was not\", \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \n",
    "                       \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\", \"we've\": \"we have\", \"weren't\": \n",
    "                       \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\",  \n",
    "                       \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \"when've\": \"when have\", \n",
    "                       \"where'd\": \"where did\", \"where's\": \"where is\", \"where've\": \"where have\", \"who'll\": \"who will\", \n",
    "                       \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\", \"why's\": \"why is\", \"why've\": \n",
    "                       \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \"won't've\": \"will not have\", \"would've\": \n",
    "                       \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \"y'all\": \"you all\", \n",
    "                       \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \n",
    "                       \"you all have\",\"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \n",
    "                       \"you will have\", \"you're\": \"you are\", \"you've\": \"you have\" }\n",
    "\n",
    "    mispell_dict = {'colour': 'color', 'centre': 'center', 'favourite': 'favorite', 'travelling': 'traveling', \n",
    "                'counselling': 'counseling', 'theatre': 'theater', 'cancelled': 'canceled', 'labour': 'labor', \n",
    "                'organisation': 'organization', 'wwii': 'world war 2', 'citicise': 'criticize', 'youtu ': 'youtube ', \n",
    "                'sallary': 'salary', 'Whta': 'What', 'narcisist': 'narcissist', 'howdo': 'how do', \n",
    "                'whatare': 'what are', 'howcan': 'how can', 'howmuch': 'how much', 'howmany': 'how many', 'whydo': \n",
    "                'why do', 'doI': 'do I', 'theBest': 'the best', 'howdoes': 'how does', 'bigdata': 'big data', '2k17': '2017', '2k18': '2018', 'qouta': 'quota', \n",
    "                'exboyfriend': 'ex boyfriend', \"whst\": 'what', 'watsapp': 'whatsapp'}\n",
    "    punct = [char for char in punctuation if char not in ['?', '!']]\n",
    "\n",
    "    #Lowering, Removing Digits, Special Characters\n",
    "    text = re.sub(\"[\\d+-/_\\\"\\(\\)]\", \"\", text.lower())\n",
    "\n",
    "    #Replacing Contractions\n",
    "    for s in specials:\n",
    "        text = text.replace(s.decode('utf-8'), \"'\")\n",
    "    text = ' '.join([contraction_mapping[t] if t in contraction_mapping \n",
    "                                            else t for t in text.split(\" \")])\n",
    "    #Replacing Mispells\n",
    "    for word in mispell_dict.keys():\n",
    "        text = text.replace(word, mispell_dict[word])\n",
    "\n",
    "    #Removing Punctuation and Lemmatizing\n",
    "    words = text.split()     \n",
    "    tagged = nltk.pos_tag(words) \n",
    "    meaningful_words=[]\n",
    "    for word, tag in tagged:\n",
    "        if word in punct:\n",
    "            continue\n",
    "        wntag = get_wordnet_pos(tag)\n",
    "        if wntag is None:\n",
    "            meaningful_words.append(WordNetLemmatizer().lemmatize(word))\n",
    "        else:\n",
    "            meaningful_words.append(WordNetLemmatizer().lemmatize(word, pos=wntag))\n",
    "    return(\" \".join( meaningful_words ))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['treated_text'] = train['text'].apply(lambda x: preprocess(x))\n",
    "test['treated_text'] = test['text'].apply(lambda x: preprocess(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lowering "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['lowered_text'] = train['text'].apply(lambda x: x.lower())\n",
    "test['lowered_text'] = test['text'].apply(lambda x: x.lower())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing Digits and Special Characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_digits(text):\n",
    "    return re.sub(\"[\\d+-/_\\\"]\", \"\", text.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['treated_text'] = train['lowered_text'].apply(lambda x: remove_digits(x))\n",
    "test['treated_text'] = test['lowered_text'].apply(lambda x: remove_digits(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Replacing Contractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "contraction_mapping = {\"n't\": \"not\", \"'t\": \"not\", \"'d\": \"would\", \"'ll\": \"will\", \"'s\": \"is\", \n",
    "                       \"'ve\": \"have\", \"'m\": \"am\", \"'re\": \"are\"}\n",
    "\n",
    "def clean_contractions(text, mapping):\n",
    "    specials = [\"’\", \"‘\", \"´\", \"`\"]\n",
    "    for s in specials:\n",
    "        text = text.replace(s, \"'\")\n",
    "    text = ' '.join([mapping[t] if t in mapping else t for t in text.split(\" \")])\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['treated_text'] = train['treated_text'].apply(lambda x: clean_contractions(x, contraction_mapping))\n",
    "test['treated_text'] = test['treated_text'].apply(lambda x: clean_contractions(x, contraction_mapping))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Replacing Mispells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "mispell_dict = {'colour': 'color', 'centre': 'center', 'favourite': 'favorite', 'travelling': 'traveling'}\n",
    "\n",
    "def correct_spelling(x, dic):\n",
    "    for word in dic.keys():\n",
    "        x = x.replace(word, dic[word])\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['treated_text'] = train['treated_text'].apply(lambda x: correct_spelling(x, mispell_dict))\n",
    "test['treated_text'] = test['treated_text'].apply(lambda x: correct_spelling(x, mispell_dict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing Punctuation and Lemmatizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "punctuation = [char for char in punctuation if char not in ['?', '!']]\n",
    "\n",
    "def final_prep(text):\n",
    "    words = text.split()               \n",
    "    meaningful_words = [WordNetLemmatizer().lemmatize(w) for w in words if w not in punctuation]   \n",
    "    return(\" \".join( meaningful_words ))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['treated_text'] = train['treated_text'].apply(lambda x: final_prep(x))\n",
    "test['treated_text'] = test['treated_text'].apply(lambda x: final_prep(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i find the lack of entertaining game on this phone quite disturbing \n",
      "\n",
      "i have ice age and it keep telling me no disc \n",
      "\n",
      "the ear bud that come with it look cheap but the sound quality is amazing \n",
      "\n",
      "all the good review for the sd are true \n",
      "\n",
      "the machine itself seems fine but the software that came with it is awful \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for q in train['treated_text'].sample(5):\n",
    "    print q, '\\n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i love this camera \n",
      "\n",
      "i did have to put a little work into renaming some duplicate file name to get all my music on my zen xtra but it wa not a big problem \n",
      "\n",
      "pro large hard drive for the gb and gb are both affordable \n",
      "\n",
      "compared to musicmatch the software ha a better filing system and easier to use \n",
      "\n",
      "there seem to be fewer collisons and dropped packet a i read from the router log than with my old dlink router \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for q in test['treated_text'].sample(5):\n",
    "    print q, '\\n'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifier Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CountVec + Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_lr_pipe = Pipeline([\n",
    "    (\"vectorizer\", CountVectorizer(analyzer='word', ngram_range=(1,3))),\n",
    "    (\"classifier\", LogisticRegression(class_weight='balanced'))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Bai/anaconda2/lib/python2.7/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7878966931043319 0.006490023507407363\n"
     ]
    }
   ],
   "source": [
    "count_lr_cv = cross_val_score(count_lr_pipe, train['treated_text'], train['target'], scoring='accuracy', cv=5)\n",
    "print count_lr_cv.mean(), count_lr_cv.std()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tf-Idf + Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_lr_pipe = Pipeline([\n",
    "    (\"vectorizer\", TfidfVectorizer(analyzer='word', ngram_range=(1,3))),\n",
    "    (\"classifier\", LogisticRegression(class_weight='balanced'))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7863891868074175 0.005966513656341954\n"
     ]
    }
   ],
   "source": [
    "tfidf_lr_cv = cross_val_score(tfidf_lr_pipe, train['treated_text'], train['target'], scoring='accuracy', cv=5)\n",
    "print tfidf_lr_cv.mean(), tfidf_lr_cv.std()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tf-Idf + Logistic Regression + StopWords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "stops = set(stopwords.words(\"english\"))  \n",
    "\n",
    "tfidf_lr_s_pipe = Pipeline([\n",
    "    (\"vectorizer\", TfidfVectorizer(analyzer='word', ngram_range=(1,3), stop_words=stops)),\n",
    "    (\"classifier\", LogisticRegression(class_weight='balanced'))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7513515365721036 0.016547232790117102\n"
     ]
    }
   ],
   "source": [
    "tfidf_lr_s_cv = cross_val_score(tfidf_lr_s_pipe, train['treated_text'], train['target'], scoring='accuracy', cv=5)\n",
    "print tfidf_lr_s_cv.mean(), tfidf_lr_s_cv.std()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tf-Idf + LinearSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_svc_pipe = Pipeline([\n",
    "    (\"vectorizer\", TfidfVectorizer(analyzer='word', ngram_range=(1,3))),\n",
    "    (\"classifier\", LinearSVC(C=2.3))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7928917305733161 0.007446575777154651\n"
     ]
    }
   ],
   "source": [
    "tfidf_svc_cv = cross_val_score(tfidf_svc_pipe, train['treated_text'], train['target'], scoring='accuracy', cv=5)\n",
    "print tfidf_svc_cv.mean(), tfidf_svc_cv.std()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Tf-Idf + Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_rf_pipe = Pipeline([\n",
    "    (\"vectorizer\", TfidfVectorizer(analyzer='word', ngram_range=(1,3))),\n",
    "    (\"classifier\", RandomForestClassifier(n_estimators = 50, class_weight='balanced'))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7413602647516548 0.011694874588635312\n"
     ]
    }
   ],
   "source": [
    "tfidf_rf_cv = cross_val_score(tfidf_rf_pipe, train['treated_text'], train['target'], scoring='accuracy', cv=5)\n",
    "print tfidf_rf_cv.mean(), tfidf_rf_cv.std()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "ans = pd.read_csv('products_sentiment_sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tfidf_svc_pipe.fit(train['treated_text'], train['target'])\n",
    "ans['y'] = model.predict(test['treated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "ans.to_csv('answer.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(analyzer='word', ngram_range=(1,3))\n",
    "vectorizer = vectorizer.fit(train['treated_text'], train['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(vectorizer, open('vectorizer.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearSVC(C=2.3)\n",
    "model = model.fit(vectorizer.transform(train['treated_text']), train['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(model, open('model.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# More data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import movie_reviews\n",
    " \n",
    "negids = movie_reviews.fileids('neg')\n",
    "posids = movie_reviews.fileids('pos')\n",
    "\n",
    "negfeats = [\" \".join(movie_reviews.words(fileids=[f])) for f in negids]\n",
    "posfeats = [\" \".join(movie_reviews.words(fileids=[f])) for f in posids]\n",
    "\n",
    "texts = negfeats + posfeats\n",
    "labels = [0] * len(negfeats) + [1] * len(posfeats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "mv_data = pd.DataFrame(\n",
    "    {'text': texts,\n",
    "     'target': labels\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "mv_data['treated_text'] = mv_data['text'].apply(lambda x: preprocess(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(analyzer='word', ngram_range=(1,3))\n",
    "vectorizer = vectorizer.fit(mv_data['treated_text'], mv_data['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Bai/anaconda2/lib/python2.7/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "model = LogisticRegression(C=2.3)\n",
    "model = model.fit(vectorizer.transform(mv_data['treated_text']), mv_data['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(vectorizer, open('vectorizer_mv.pkl', 'wb'))\n",
    "pickle.dump(model, open('model_mv.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.7975, 0.7925, 0.8175, 0.8275, 0.8275])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = cross_val_score(model,vectorizer.transform(mv_data['treated_text']), mv_data['target'], scoring='accuracy', cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of texts with the target == 1 :  1000\n",
      "The percentage : 50.0\n",
      "\n",
      "Random 5 samples:\n",
      "[u'assume nothing . the phrase is perhaps one of the most used of the 1990 \\' s , as first impressions and rumors are hardly ever what they seem to be . the phrase especially goes for oscar novak , an architect who is the main focus of three to tango , a delightful , funny romantic comedy about assumptions and being yourself . novak ( matthew perry ) , a shy , clumsy , chicago based architect , along with openly gay partner , peter steinberg ( oliver platt ) , fights for projects day in and day out . one of these is the job of restoring a popular building for charles newman ( dylan mcdermott ) , a rich , well - known businessman . charles immediately takes a liking to oscar , as he enjoys his personality and sense of humor . seeing oscar as someone he could trust , charles asks him to watch his girlfriend , an unpredictable , adventurous girl named amy post ( neve campbell ) , who makes a living by blowing glass . charles wants to know who she talks to , what she does , and where she goes . the point ? to make sure she \\' s not seeing someone else , of course . oscar gladly takes the job , and meets amy at an art show of hers , and sparks fly between the two from the get go . oscar feels he has found the one meant for him , and he is content with the idea of being with amy . well , another popular phrase of the 90 \\' s is \" all good things must come to an end , \" and this stays true for oscar as well . charles walks in on amy and oscar having a drink one night , as oscar and amy have become great friends , but he doesn \\' t seem to mind . why is this ? he thinks oscar is gay . he \\' s not afraid to share this with him either . oscar stands in shock after the words , \" i swear if you weren \\' t gay oscar , i \\' d have to kill you , \" are muttered flamboyantly from charles \\' mouth . the word spreads instantly through town . will oscar \" come out \" of his supposed gayness , or will he tell everyone that he isn \\' t ? one would immediately think he would deny the fact , but numerous occurrences come to oscar , which result in the fact that if he denies the fact , he could lose his job with charles . matthew perry doesn \\' t escape his character as chandler on the already classic t . v . comedy \" friends , \" as both oscar novak and chandler are clueless , shy , and sensitive . nonetheless , perry is hilarious here , and shows that he can handle drama , as obviously , his character suffers quite a bit here . it \\' s wonderful to see neve campbell outside of a horror movie , ( she was the star of scream 1 and 2 , and the upcoming scream 3 ) as she handles comedy superbly here . her voice , smile , and personality are more than perfect for romantic comedies - stay with this genre , neve . neve is delightful as her conflicted character , who feels love for oscar , but knows , based on rumors , that he is gay . as usual , campbell is likable as her likable character . unlike the other two leads here , dylan mcdermott is flat in his dialogue , and is never convincing . when his character his present , mcdermott sets a dull tone to the scene with his horrible acting . stick to the t . v . drama , the practice , dylan . the major weak spot in three to tango is the direction of damon santostefano . no originality or technique is used whatsoever . three to tango is lucky that the script is so edgy and that perry and campbell are wonderful in comedy , or else the film would have been a disaster , as it is just plain boring to look at . three to tango is a film done many times before , as the plot is suspiciously close to 1998 \\' s the object of my affection , but the plot has never been completed so well . three to tango \\' s script , written by rodney patrick vaccaro and aline brosh mckenna is fun , fast , and funny , delivering not only original , hilarious gay jokes ( not your run of the mill material ) , but a certain snappiness in the dialogue between characters that always keeps you smiling . unlike last summer \\' s south park : bigger , longer , and uncut and 1997 \\' s in & out , three to tango is a comedy in which the gay element is not crude or vulgar . the script is wise to take this route , as gays can \\' t ( i don \\' t think so , anyway ) be offended by this light , playful comedy . ( to prove this , a gay couple was in the audience who were laughing constantly . ) three to tango \\' s climax is a hilarious , clever scene that is pure irony based on the outcome of most romantic - mistaken identity comedies . three to tango is a gem . the bottom line : three to tango is a light , sharp , snappy romantic comedy with a superb ending , and great stars . one of the better romantic comedies of 1999 .']\n",
      "[u'charlie sheen stars as zane , a radio astronomer who listens for sounds from other lifeforms . when he finally gets one , his boss destroys the tape and fires him . naturally , zane is not ready to give up , and he comes up with an ingenious way to do this himself . he is aided by a young neighborhood kid and they discover that the sound is coming from mexico . so zane goes down there to investigate , and runs into a lady studying why the temperature of the earth has dangerously risen so suddenly . zane is having marital problems at the time , and an offer by her to spend the night with him is very tempting . hearing charlie sheen deliver the line , \" i guess there is something to be said for celibacy \" is the funniest thing i have ever heard in a movie since matthew broderick discussed asexual reproduction in wargames . this is just the setup , and i don \\' t want to give too much away , because a large part of the movies fun is the surprises . charlie sheen , who has had a rocky career as of late , is in top form here . he is funny , serious , and determined to accomplish his goal . sheen \\' s absolutely terrific performance is another big plus to this movie . the story is ingeniously devised by twohy , who also wrote and directed the equally clever cable movie grand tour : disaster in time . the films major flaw is a very slow pace , and not much happens in the earlygoings . viewers may be growing restless for a while , but trust me if you stick around and keep your head in it , you will have a good time .']\n",
      "[u'trees lounge is the directoral debut from one of my favorite actors , steve buscemi . he gave memorable performences in in the soup , fargo , and reservoir dogs . now he tries his hand at writing , directing and acting all in the same flick . the movie starts out awfully slow with tommy ( buscemi ) hanging around a local bar the \" trees lounge \" and him pestering his brother . it \\' s obvious he a loser . but as he says \" it \\' s better i \\' m a loser and know i am , then being a loser and not thinking i am . \" well put . the story starts to take off when his uncle dies , and tommy , not having a job , decides to drive an ice cream truck . well , the movie starts to pick up with him finding a love interest in a 17 year old girl named debbie ( chloe sevigny ) and . . . i liked this movie alot even though it did not reach my expectation . after you \\' ve seen him in fargo and reservoir dogs , you know he is capable of a better performence . i think his brother , michael , did an excellent job for his debut performence . mr . buscemi is off to a good career as a director !']\n",
      "[u'plot : during a 10 - week span in london in the fall of 1888 , jack the ripper , a man whose identity was never uncovered , committed five ritualistic , grisly murders of prostitutes . this film is based on the graphic novel which assembled a theory behind the century - old mystery . critique : awesome ! i love dark movies , i love stylish movies , i love good mysteries and i love johnny depp . . . so yes , i really dug this movie ! the hughes brothers are back with a vengeance , out to show the world that they are directors to be taken seriously , and this movie , my friends . . . is very serious ! in fact , it \\' s also very gruesome and bloody and morbid and creepy and black as hell . it ain \\' t for the kids , i \\' ll tell you that much . and if any of those recent hack directors of two - bit scream - esque slasher flicks want some advice on effective \" kill scenes \" , do not look any further , because each one committed to this film is a cinematic stunner , especially when compared to their generic slice - and - dice contrivances . in fact , most of this film \\' s structure is very creative , the actors , all solid , the atmosphere and \" feel \" of the picture , remarkably real ( gotta love those \" unfortunates \" ) , and the descent into darkness , well honed by the filmmakers . you can almost taste the seediness off the screen and quickly come to appreciate the politics of the time . but it \\' s also an impressive mystery film , based on the true - life tale of jack the ripper , a man whose identity was never discovered . the theory in this film is based on the graphic novel \" from hell \" written by alan moore and presents a compelling story , which despite being a little oliver stone - ish , did manage to tie up all of its loose ends quite nicely , and present us with an entertaining bowl of mayhem , sexual depravity , drug usage , insanity , murders and love ? yes , the brothers even managed to slip a little \" love \" story in here , and despite not truly being developed , i guess it could \\' ve been much worse and taken up more time than was provided already . that \\' s right , there \\' s no real need to put any \" romance \" in a film about a mass murderer ! but that aside , you gotta give up the props to actor extraordinaire johnny depp , who once again possesses the strength in character and flawless acting abilities , to center this movie all the way through . sure , he \\' s just playing a less goofier version of his ichabod crane character from sleepy hollow , but he does it so well and even added a michael caine accent to his mixed bag of tricks in this one . i was also surprised at how little heather graham annoyed me in this film ( i was sure that her presence would ruin this movie , but she was fine ) and how little the dreams / fantasies of depp \\' s character had to do with the film \\' s development ( from what i had seen in the trailer , i thought the filmmakers were going to make this the crux of the story , but thankfully , it wasn \\' t ) . on the downside , the film doesn \\' t really explain the whole freemasons cult organization real well and lacks character development . but then again , a story about the most mysterious serial killer of all - time is probably not the ideal spot for character refinement . i mean , the less i knew about each person , the more suspicious i became of each one \\' s motives , reasons , etc . . . in fact , i was actually quite surprised by who was presented as the killer in the end , and that was in part , due to the distance to which i \\' d been held from the film \\' s characters . . . or am i giving the filmmakers too much credit for something that might not have been their intention ? perhaps . overall , this is an extremely creepy production , with glorious cinematography ( reminiscent of coppola \\' s dracula and burton \\' s sleepy hollow ) , a grim score and solid acting . but it \\' s not for everyone . some may be utterly repulsed by the gory particulars of these tragic events , and even though the murders aren \\' t shown as graphically as you might think , there \\' s enough here to send the squeamish right out the doors . but i for one love movies that blend their eclectic visual style with an interesting mystery and palpable , gothic atmosphere , and this flick is just that and plenty more ! and god knows that i will certainly not be surrendering the burnt image of jack the ripper \\' s haunting cape silhouette and gleaming silver knife from my mind anytime soon . . . brrrrr ! where \\' s joblo coming from ? the crow ( 9 / 10 ) - dark city ( 9 / 10 ) - dracula ( 7 / 10 ) - jfk ( 9 / 10 ) - the nightmare before christmas ( 10 / 10 ) - the ninth gate ( 8 / 10 ) - the others ( 9 / 10 ) - quills ( 8 / 10 ) - shadow of the vampire ( 6 / 10 ) - sleepy hollow ( 8 / 10 )']\n",
      "[u'hollywood is a pimp . a fat , cigar - smoking chump wearing a fur hat and 12 gold chains around its fat , hairy chest . all of its stars and starlets are an evil brood of scum - sucking vampires looking for the next percentage take , the next summer blockbuster , the next casting couch to audition on . pumping out comic - book adaptations , terrible sequels to mediocre films , and remakes of foreign films to the nearest american movie multiplex mall theater equipped with thin walls and bad sound systems . how much longer can the works of peckinpah , fassbinder , fuller , castle , preminger , and lee be placed and forgotten in the wrong sections of the local blockbuster stores ? how many more silver and weinstein films can we enduring in this stinky , decaying state of american cinema ? but now , from john \" i don \\' t give a shit what you think about my movies \" waters , comes the siren call to all frustrated filmmakers and aficionados : cecil b . demented , a warped and twisted tale of how far a filmmaker will go to create a personal vision of internal and social revolution . stephen dorff , in a career - defining role , is cecil b . demented , a crazed director devoted to making the most radical underground film . together with his film production cult , the sprocket holes - who wear tattoos of peckinpah , lee , fuller , castle , anger , fassbinder , preminger on various parts of their bodies as badges of honor , they kidnap a hollywood movie starlet , played with perfect ridiculousness by melaine griffith , and force her to take the starring role in demented \\' s film . with no budget and no contracts for extras , demented and his crew take to the streets for production of raving beauty , a crass terrorist film about an angry owner of an independent theater and her brood out to destroy the mainstream film business . using \" ultimate reality \" - with real bullets , real people , and real terror - demented and his crew of misfits attack a mall theater , terrorize the maryland film commission and crash a movie studio shooting a certain sequel to a really annoying tom hanks film . demented \\' s crewmembers are maimed and killed , popcorn machines are used for target practice , and no one can have sex until the film is complete . it \\' s like bowfinger , only , you know , good . the film moves with zigs and zags like the magic bullet of kennedy \\' s assassination . the zeal of demented \\' s cause catches quickly and conveys the urgent message of \" doing something , anything , for the accomplishment of artistic motivations . \" the crewmembers all hold the quirkiness common in water \\' s previous films - pink flamingos , hairspray , polyester , pecker -- and speak in the choppy , jaded dialogue used frequently by waters . it is as if water \\' s script strips away the unnecessary dialogue common to most pretentious indie films and just delivers the goods . cecil takes such warped avenues of expression that it seems like it might actually outdo itself . you can see how a major studio might take this film , re - edit it , cut a deal with the remaining crew members who are still alive , and make a few sequels , a la the blair witch project . but that \\' s for the future . overall , the ride is fantastic ; it \\' s one of water \\' s best films to date and this year \\' s fight club for filmmakers . director / writer : john waters starring : stephen dorff , melaine griffith , alicia witt , adrian grenier , larry gilliard jr . , mink stole , ricki lake , kevin nealon producers : john fielder , mark tarlov , anthony delorenzo , joe caraccio jr .']\n"
     ]
    }
   ],
   "source": [
    "print 'Number of texts with the target == 1 : ', mv_data[mv_data['target']==0].shape[0]\n",
    "print ('The percentage : %0.1f' % (100.*mv_data[mv_data['target']==0].shape[0]/mv_data.shape[0]))\n",
    "print '\\nRandom 5 samples:'\n",
    "for t in mv_data.loc[mv_data.target==1, ['text']].sample(5).values:\n",
    "    print t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "256px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
